{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge: Neural Network #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## By: Lorenz Madarang ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "import operator\n",
    "from sklearn import linear_model\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Display preferences.\n",
    "%matplotlib inline\n",
    "pd.options.display.float_format = '{:.3f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingesting Las Vegas Restaurant Inspections Data ###\n",
    "This data is from the same one I used for my Mid-Course Capstone.  I will be building a neural network on this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "restaurant=pd.read_csv('Restaurant_Inspections2Copy.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the Data and Feature Selection ###\n",
    "I utilized the same data cleaning steps that I did for my Mid-Course Capstone.  I created a column that had boolean values that identified whether a restaurant received a downgrade in grade or not.  I also converted all the of the time variables to datetime objects.  Also, I created seperate variables that denote the year, month, and hour of the inspection.  Also, I conducted a feature selection process that limited the variables to the 10 most significant variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create an empty list that will hold the boolean expression indicating a downgrade\n",
    "downgrade_cat = []\n",
    "\n",
    "#Iterate through Dataframe \n",
    "for index, row in restaurant.iterrows():\n",
    "    #Use regex to see if 'Downgrade' is in the 'Inspection Result' column\n",
    "    #Append downgrade_cat with '1' if present, '0' otherwise\n",
    "    if re.match('.*(Downgrade)',str(row['Inspection Result'])):\n",
    "        downgrade_cat.append(1)\n",
    "    else:\n",
    "        downgrade_cat.append(0)\n",
    "\n",
    "#Create new boolean column indicating downgrade or not\n",
    "restaurant['Downgrade'] = downgrade_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Convert 'Inspection Time' and 'Inspection Date' to datetime object\n",
    "restaurant[\"Inspection Time\"] = pd.to_datetime(restaurant[\"Inspection Time\"])\n",
    "restaurant[\"Inspection Date\"] = pd.to_datetime(restaurant[\"Inspection Date\"])\n",
    "\n",
    "#Create empty list that will hold the year, month, and hour values\n",
    "year = []\n",
    "month = []\n",
    "hour = []\n",
    "\n",
    "#Iterate through Dataframe\n",
    "for index, row in restaurant.iterrows():\n",
    "    year.append(row['Inspection Time'].year)\n",
    "    month.append(row['Inspection Time'].month)\n",
    "    hour.append(row['Inspection Time'].hour)\n",
    "    \n",
    "restaurant['Year'] = year\n",
    "restaurant['Month'] = month\n",
    "restaurant['Hour'] = hour  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Sort the restaurant data so that the oldest inspection date is at the top\n",
    "inspectdate_sort = restaurant.sort_values('Inspection Date', ascending=True)\n",
    "\n",
    "#Create a Cumulative count column that keeps a count a restaurant's past downgrades\n",
    "inspectdate_sort[\"Cum_Count\"] = inspectdate_sort.groupby(['Restaurant Name'])['Downgrade'].apply(lambda x: x.cumsum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create dataframe does not have any null values for \"Cum_Count\"\n",
    "inspectdate_sort = inspectdate_sort[pd.notnull(inspectdate_sort['Cum_Count'])]\n",
    "\n",
    "#Reset the index\n",
    "inspectdate_sort = inspectdate_sort.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inspectdate_sort['Year'] = inspectdate_sort['Year'].astype('category')\n",
    "inspectdate_sort['Month'] = inspectdate_sort['Month'].astype('category')\n",
    "inspectdate_sort['Hour'] = inspectdate_sort['Hour'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create feature dataset \n",
    "X = inspectdate_sort[['Category Name', 'City', 'Zip', 'Current Demerits', 'Current Grade', 'Inspection Type', 'Cum_Count' ]]\n",
    "\n",
    "#Get numerical values for the categorical variables\n",
    "X = pd.get_dummies(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create Target column\n",
    "Y = inspectdate_sort['Downgrade']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Use train_test_split to create the necessary training and test groups\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lorenzmadarang/anaconda3/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [  47  157  214  284  301  430  553  696  787 1025 1034 1086 1441 1564 1590\n",
      " 1702 1812 1817 2048 2124 2261 2425 2523 2524 2655 2856 3287 3557 3611] are constant.\n",
      "  UserWarning)\n",
      "/Users/lorenzmadarang/anaconda3/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SelectKBest(k=10, score_func=<function f_classif at 0x1a0d661400>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "sel = SelectKBest()\n",
    "\n",
    "sel.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Apply Feature Selection on the train and test data\n",
    "\n",
    "X_train = sel.transform(X_train)\n",
    "X_test = sel.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network ###\n",
    "A neural network was run on the restaurant inspection data.  For this neural network, I used a single 1000 perceptron layer.  Just like my Mid-Course capstone, the method of evaluation is not the accuracy of the model but the precision of the model.  The neural network it is not as accurate as the gradient search optimzed Random Forest model from my Mid-Course Capstone.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(1000,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the model.\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Establish and fit the model, with a single, 1000 perceptron layer.\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(1000,))\n",
    "mlp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89090266875981161"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict_test = mlp.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity is 0.5240384615384616\n",
      "Specificity is 0.8929503085830629\n"
     ]
    }
   ],
   "source": [
    "#Sensitivity and Specificity Score on the test dataset\n",
    "from sklearn.metrics import confusion_matrix\n",
    "matrix = confusion_matrix(y_test, predict_test)\n",
    "print('Sensitivity is {}'.format(matrix[1,1]/(matrix[0,1]+matrix[1,1])))\n",
    "print('Specificity is {}'.format(matrix[0,0]/(matrix[0,0]+matrix[1,0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Precision Scores - Test Set: 0.60537(+/- 0.23)\n",
      "Cross Validation Precision Scores - Train Set: 0.56519(+/- 0.09)\n"
     ]
    }
   ],
   "source": [
    "#Cross-validation Precision Scores on the test and train dataset from Best GB GridSearch parameters\n",
    "from sklearn.model_selection import cross_val_score\n",
    "precision_MLPtest = cross_val_score(mlp, X_test, y_test, cv=10, scoring='precision')\n",
    "print('Cross Validation Precision Scores - Test Set: {:.5f}(+/- {:.2f})'.format(precision_MLPtest.mean(), \n",
    "                                                                               precision_MLPtest.std()*2))\n",
    "precision_MLPtrain = cross_val_score(mlp, X_train, y_train, cv=10, scoring='precision')\n",
    "print('Cross Validation Precision Scores - Train Set: {:.5f}(+/- {:.2f})'.format(precision_MLPtrain.mean(), \n",
    "                                                                               precision_MLPtrain.std()*2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision Scores from Random Forest Model ###\n",
    "Cross Validation Precision Scores - Training Set: 0.636(+/- 0.12)\n",
    "\n",
    "Cross Validation Precision Scores - Test Set: 0.659(+/- 0.23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
