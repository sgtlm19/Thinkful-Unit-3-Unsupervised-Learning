{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Challenge Identifying Adele & Bob Marley Lyrics #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## by Lorenz Madarang ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data: https://www.kaggle.com/paultimothymooney/poetry/kernels ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview ##\n",
    "Two .txt files that contain lyrics to Adele songs and Bob Marley songs are ingested.  Supervised BoW models are run on on the corpus of lyrics and an unsupervised vector space model is used to find features.  Natural Language Processing has been applied to extract context from the corpus and to remove stop words.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lorenzmadarang/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: 'U' mode is deprecated\n",
      "  \n",
      "/Users/lorenzmadarang/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: 'U' mode is deprecated\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Read in the Adele Lyrics \n",
    "f=open('adele.txt','rU')\n",
    "adele=f.read()\n",
    "\n",
    "# Read in the Bob Marley Lyrics\n",
    "f=open('bob-marley.txt','rU')\n",
    "marley=f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "# Apply spaCy nlp to the lyric corpus\n",
    "adele_doc = nlp(adele)\n",
    "marley_doc = nlp(marley)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The adele_doc object is a <class 'spacy.tokens.doc.Doc'> object.\n",
      "It is 23858 tokens long\n",
      "The first three tokens are 'Looking for some'\n"
     ]
    }
   ],
   "source": [
    "# Take a peak at the token information of the Adele Lyrics\n",
    "print(\"The adele_doc object is a {} object.\".format(type(adele_doc)))\n",
    "print(\"It is {} tokens long\".format(len(adele_doc)))\n",
    "print(\"The first three tokens are '{}'\".format(adele_doc[:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Frequencies ###\n",
    "Word frequencies have been conducted on both lyrics.  Two versions of the word frequencies were created one that includes stop words and the other one that does not.  Newline characters were captured in both frequencies and weird contractions like \"n't\" and \"'s\".  In the word frequency that excludes stopwords more meaningful words were able to come through such as \"love\" and \"You\".  The unique words for each corpus are contractions.  It is also interesting to note that 'You' is unique to Adele but 'you' is used in Bob Marley songs.  It looks like the word frequencies are case sensitive.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adele: [('\\n', 2399), ('I', 1249), ('you', 854), ('the', 590), ('me', 484), ('it', 429), ('to', 403), (\"n't\", 396), ('my', 358), ('your', 254)]\n",
      "Bob Marley: [('\\n', 2217), ('I', 596), ('the', 547), ('you', 511), ('a', 371), ('to', 295), (\"n't\", 275), ('it', 260), ('no', 238), ('be', 194)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Utility function to calculate how frequently words appear in the text.\n",
    "def word_frequencies(text, include_stop=True):\n",
    "    \n",
    "    # Build a list of words.\n",
    "    # Strip out punctuation and, optionally, stop words.\n",
    "    words = []\n",
    "    for token in text:\n",
    "        if not token.is_punct and (not token.is_stop or include_stop):\n",
    "            words.append(token.text)\n",
    "            \n",
    "    # Build and return a Counter object containing word counts.\n",
    "    return Counter(words)\n",
    "    \n",
    "# The most frequent words:\n",
    "adele_freq = word_frequencies(adele_doc).most_common(10)\n",
    "marley_freq = word_frequencies(marley_doc).most_common(10)\n",
    "print('Adele:', adele_freq)\n",
    "print('Bob Marley:', marley_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adele: [('\\n', 2399), ('I', 1249), (\"n't\", 396), ('love', 226), ('You', 197), ('And', 192), (\"'s\", 171), (\"'m\", 163), (\"'re\", 145), (\"'ve\", 129)]\n",
      "Bob Marley: [('\\n', 2217), ('I', 596), (\"n't\", 275), (\"'s\", 184), ('love', 164), ('yeah', 144), ('na', 142), ('We', 122), ('And', 119), ('oh', 117)]\n"
     ]
    }
   ],
   "source": [
    "# Use our optional keyword argument to remove stop words.\n",
    "adele_freq = word_frequencies(adele_doc, include_stop=False).most_common(10)\n",
    "marley_freq = word_frequencies(marley_doc, include_stop=False).most_common(10)\n",
    "print('Adele:', adele_freq)\n",
    "print('Bob Marley:', marley_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique to Adele: {\"'m\", \"'ve\", 'You', \"'re\"}\n",
      "Unique to Bob Marley: {'We', 'na', 'yeah', 'oh'}\n"
     ]
    }
   ],
   "source": [
    "# Pull out just the text from our frequency lists.\n",
    "adele_common = [pair[0] for pair in adele_freq]\n",
    "marley_common = [pair[0] for pair in marley_freq]\n",
    "\n",
    "# Use sets to find the unique values in each top ten.\n",
    "print('Unique to Adele:', set(adele_common) - set(marley_common))\n",
    "print('Unique to Bob Marley:', set(marley_common) - set(adele_common))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemma Frequencies ###\n",
    "The lemma frequencies for each lyric corpus are pretty similar: \"be,\" \"not,\" and \"love\".  The unique lemmas for each corpus are interesting.  The lemmas might imply overall themes for their songs.  The most common and unique lemma to Adele lyrics is \"know.\"  It can be taken that Adele wants her subject or audience to constantly 'know' something about her in her songs.  The most common and unique lemma to Bob Marley lyrics is \"come.\" It can be taken that Bob Marley is either inviting his subjects in his songs or talking about a subject that has traversed.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adele: [('\\n', 2399), ('-PRON-', 1711), ('be', 585), ('not', 396), ('love', 254), ('and', 192), ('know', 163), ('will', 151), ('have', 137), ('let', 133)]\n",
      "Bob Marley: [('\\n', 2217), ('-PRON-', 945), ('be', 375), ('not', 277), ('love', 230), ('oh', 204), ('yeah', 159), ('go', 154), ('come', 149), ('to', 138)]\n",
      "Unique to Adele: {'know', 'let', 'have', 'will', 'and'}\n",
      "Unique to Bob Marley: {'to', 'come', 'oh', 'yeah', 'go'}\n"
     ]
    }
   ],
   "source": [
    "# Utility function to calculate how frequently lemmas appear in the text.\n",
    "def lemma_frequencies(text, include_stop=True):\n",
    "    \n",
    "    # Build a list of lemas.\n",
    "    # Strip out punctuation and, optionally, stop words.\n",
    "    lemmas = []\n",
    "    for token in text:\n",
    "        if not token.is_punct and (not token.is_stop or include_stop):\n",
    "            lemmas.append(token.lemma_)\n",
    "            \n",
    "    # Build and return a Counter object containing word counts.\n",
    "    return Counter(lemmas)\n",
    "\n",
    "# Instantiate our list of most common lemmas.\n",
    "adele_lemma_freq = lemma_frequencies(adele_doc, include_stop=False).most_common(10)\n",
    "marley_lemma_freq = lemma_frequencies(marley_doc, include_stop=False).most_common(10)\n",
    "print('\\nAdele:', adele_lemma_freq)\n",
    "print('Bob Marley:', marley_lemma_freq)\n",
    "\n",
    "# Again, identify the lemmas common to one text but not the other.\n",
    "adele_lemma_common = [pair[0] for pair in adele_lemma_freq]\n",
    "marley_lemma_common = [pair[0] for pair in marley_lemma_freq]\n",
    "print('Unique to Adele:', set(adele_lemma_common) - set(marley_lemma_common))\n",
    "print('Unique to Bob Marley:', set(marley_lemma_common) - set(adele_lemma_common))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration of Sentences ###\n",
    "spaCy was able to identify sentences within the lyrics.  It was also able to properly identify the parts of speech and dependencies in a sentence.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adele has 2222 sentences.\n",
      "Here is an example: \n",
      "I won't bore you with the details, baby\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initial exploration of sentences.\n",
    "sentences = list(adele_doc.sents)\n",
    "print(\"Adele has {} sentences.\".format(len(sentences)))\n",
    "\n",
    "example_sentence = sentences[2]\n",
    "print(\"Here is an example: \\n{}\\n\".format(example_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Looking for some education\n",
       "Made my way into the night"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parts of speech:\n",
      "I PRON\n",
      "wo VERB\n",
      "n't ADV\n",
      "bore VERB\n",
      "you PRON\n",
      "with ADP\n",
      "the DET\n",
      "details NOUN\n",
      ", PUNCT\n"
     ]
    }
   ],
   "source": [
    "# View the part of speech for some tokens in our sentence.\n",
    "print('\\nParts of speech:')\n",
    "for token in example_sentence[:9]:\n",
    "    print(token.orth_, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dependencies:\n",
      "I nsubj bore\n",
      "wo aux bore\n",
      "n't neg bore\n",
      "bore ROOT bore\n",
      "you dobj bore\n",
      "with prep bore\n",
      "the det details\n",
      "details pobj with\n",
      ", punct bore\n"
     ]
    }
   ],
   "source": [
    "# View the dependencies for some tokens.\n",
    "print('\\nDependencies:')\n",
    "for token in example_sentence[:9]:\n",
    "    print(token.orth_, token.dep_, token.head.orth_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words Model ##\n",
    "A dataframe was created of the lyric sentences and its associated singer.  A bag of words was created for each singer's lyric corpus and then a combined bag of words was created.  And then a dataframe was created with the combined bag of words as features.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(Looking, for, some, education, \\n, Made, my, ...</td>\n",
       "      <td>Adele</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(All, that, bullshit, conversation, \\n, Baby, ...</td>\n",
       "      <td>Adele</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(I, wo, n't, bore, you, with, the, details, ,,...</td>\n",
       "      <td>Adele</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(I, do, n't, even, wanna, waste, your, time, \\n)</td>\n",
       "      <td>Adele</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(Let, 's, just, say, that, maybe, \\n, You, cou...</td>\n",
       "      <td>Adele</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0      1\n",
       "0  (Looking, for, some, education, \\n, Made, my, ...  Adele\n",
       "1  (All, that, bullshit, conversation, \\n, Baby, ...  Adele\n",
       "2  (I, wo, n't, bore, you, with, the, details, ,,...  Adele\n",
       "3   (I, do, n't, even, wanna, waste, your, time, \\n)  Adele\n",
       "4  (Let, 's, just, say, that, maybe, \\n, You, cou...  Adele"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group into sentences.\n",
    "adele_sents = [[sent, \"Adele\"] for sent in list(adele_doc.sents)]\n",
    "marley_sents = [[sent, \"Bob Marley\"] for sent in list(marley_doc.sents)]\n",
    "\n",
    "# Combine the sentences from the two lyric corpus into one data frame.\n",
    "sentences = pd.DataFrame(adele_sents + marley_sents)\n",
    "sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Utility function to create a list of the 2000 most common words.\n",
    "def bag_of_words(text):\n",
    "    \n",
    "    # Filter out punctuation and stop words.\n",
    "    allwords = [token.lemma_\n",
    "                for token in text\n",
    "                if not token.is_punct\n",
    "                and not token.is_stop]\n",
    "    \n",
    "    # Return the most common words.\n",
    "    return [item[0] for item in Counter(allwords).most_common(2000)]\n",
    "    \n",
    "\n",
    "# Creates a data frame with features for each word in our common word set.\n",
    "# Each value is the count of the times the word appears in each sentence.\n",
    "def bow_features(sentences, common_words):\n",
    "    \n",
    "    # Scaffold the data frame and initialize counts to zero.\n",
    "    df = pd.DataFrame(columns=common_words)\n",
    "    df['text_sentence'] = sentences[0]\n",
    "    df['text_source'] = sentences[1]\n",
    "    df.loc[:, common_words] = 0\n",
    "    \n",
    "    # Process each row, counting the occurrence of words in each sentence.\n",
    "    for i, sentence in enumerate(df['text_sentence']):\n",
    "        \n",
    "        # Convert the sentence to lemmas, then filter out punctuation,\n",
    "        # stop words, and uncommon words.\n",
    "        words = [token.lemma_\n",
    "                 for token in sentence\n",
    "                 if (\n",
    "                     not token.is_punct\n",
    "                     and not token.is_stop\n",
    "                     and token.lemma_ in common_words\n",
    "                 )]\n",
    "        \n",
    "        # Populate the row with word counts.\n",
    "        for word in words:\n",
    "            df.loc[i, word] += 1\n",
    "        \n",
    "        # This counter is just to make sure the kernel didn't hang.\n",
    "        if i % 500 == 0:\n",
    "            print(\"Processing row {}\".format(i))\n",
    "            \n",
    "    return df\n",
    "\n",
    "# Set up the bags.\n",
    "adelewords = bag_of_words(adele_doc)\n",
    "marleywords = bag_of_words(marley_doc)\n",
    "\n",
    "# Combine bags to create a set of unique words.\n",
    "common_words = set(adelewords + marleywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 500\n",
      "Processing row 1000\n",
      "Processing row 1500\n",
      "Processing row 2000\n",
      "Processing row 2500\n",
      "Processing row 3000\n",
      "Processing row 3500\n",
      "Processing row 4000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tongue</th>\n",
       "      <th>belong</th>\n",
       "      <th>picture</th>\n",
       "      <th>lust</th>\n",
       "      <th>half</th>\n",
       "      <th>ashamed</th>\n",
       "      <th>reflexes</th>\n",
       "      <th>shoot</th>\n",
       "      <th>government</th>\n",
       "      <th>despair</th>\n",
       "      <th>...</th>\n",
       "      <th>almighty</th>\n",
       "      <th>thit</th>\n",
       "      <th>path</th>\n",
       "      <th>because</th>\n",
       "      <th>crows</th>\n",
       "      <th>bring</th>\n",
       "      <th>clearly</th>\n",
       "      <th>hearts</th>\n",
       "      <th>text_sentence</th>\n",
       "      <th>text_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Looking, for, some, education, \\n, Made, my, ...</td>\n",
       "      <td>Adele</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(All, that, bullshit, conversation, \\n, Baby, ...</td>\n",
       "      <td>Adele</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(I, wo, n't, bore, you, with, the, details, ,,...</td>\n",
       "      <td>Adele</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(I, do, n't, even, wanna, waste, your, time, \\n)</td>\n",
       "      <td>Adele</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Let, 's, just, say, that, maybe, \\n, You, cou...</td>\n",
       "      <td>Adele</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1898 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  tongue belong picture lust half ashamed reflexes shoot government despair  \\\n",
       "0      0      0       0    0    0       0        0     0          0       0   \n",
       "1      0      0       0    0    0       0        0     0          0       0   \n",
       "2      0      0       0    0    0       0        0     0          0       0   \n",
       "3      0      0       0    0    0       0        0     0          0       0   \n",
       "4      0      0       0    0    0       0        0     0          0       0   \n",
       "\n",
       "      ...     almighty thit path because crows bring clearly hearts  \\\n",
       "0     ...            0    0    0       0     0     0       0      0   \n",
       "1     ...            0    0    0       0     0     0       0      0   \n",
       "2     ...            0    0    0       0     0     0       0      0   \n",
       "3     ...            0    0    0       0     0     0       0      0   \n",
       "4     ...            0    0    0       0     0     0       0      0   \n",
       "\n",
       "                                       text_sentence text_source  \n",
       "0  (Looking, for, some, education, \\n, Made, my, ...       Adele  \n",
       "1  (All, that, bullshit, conversation, \\n, Baby, ...       Adele  \n",
       "2  (I, wo, n't, bore, you, with, the, details, ,,...       Adele  \n",
       "3   (I, do, n't, even, wanna, waste, your, time, \\n)       Adele  \n",
       "4  (Let, 's, just, say, that, maybe, \\n, You, cou...       Adele  \n",
       "\n",
       "[5 rows x 1898 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create our data frame with features. This can take a while to run.\n",
    "word_counts = bow_features(sentences, common_words)\n",
    "word_counts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Models ###\n",
    "Three models where ran on the Bag of Words.  The first model that was run was a Random Forest model, the second model was a Logistic Regression model, and the final model that was run was a Gradient Boosting model.  Most of the models are pretty accurate with accuracies above 75% on the test set.  But the best model is the logistic regression model.  The accuracy on the test data is 83%.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.98223733938\n",
      "\n",
      "Test set score: 0.823129251701\n"
     ]
    }
   ],
   "source": [
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Initiate Random Forest model and create dependent and independent features\n",
    "rfc = ensemble.RandomForestClassifier()\n",
    "Y = word_counts['text_source']\n",
    "X = np.array(word_counts.drop(['text_sentence','text_source'], 1))\n",
    "\n",
    "# Test and train split \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    Y,\n",
    "                                                    test_size=0.4,\n",
    "                                                    random_state=0)\n",
    "\n",
    "# Train Random Forest model on training set\n",
    "train = rfc.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score:', rfc.score(X_train, y_train))\n",
    "print('\\nTest set score:', rfc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2646, 1896) (2646,)\n",
      "Training set score: 0.908919123205\n",
      "\n",
      "Test set score: 0.835600907029\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initiate Logistic Regression model and then run Logistic Regression model on the training data\n",
    "lr = LogisticRegression()\n",
    "train = lr.fit(X_train, y_train)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print('Training set score:', lr.score(X_train, y_train))\n",
    "print('\\nTest set score:', lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Model ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.793272864701\n",
      "\n",
      "Test set score: 0.746598639456\n"
     ]
    }
   ],
   "source": [
    "# Initiate Gradient Boosting model and then run the Gradient Boosting model on the training data\n",
    "clf = ensemble.GradientBoostingClassifier()\n",
    "train = clf.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score:', clf.score(X_train, y_train))\n",
    "print('\\nTest set score:', clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closer Look at the Best Supervised Model ###\n",
    "A Confusion matrix shows the number of correct and incorrect predictions on the test set.  Also, a cross-validation was run on the test set to make sure that no over-fitting was occuring.  The model was pretty consistent across the ten folds of cross validation with a variance of +/- 5%.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set score: 0.835600907029\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>Adele</th>\n",
       "      <th>Bob Marley</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_source</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Adele</th>\n",
       "      <td>769</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bob Marley</th>\n",
       "      <td>153</td>\n",
       "      <td>705</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0        Adele  Bob Marley\n",
       "text_source                   \n",
       "Adele          769         137\n",
       "Bob Marley     153         705"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('\\nTest set score:', lr.score(X_test, y_test))\n",
    "lr_predicted = lr.predict(X_test)\n",
    "pd.crosstab(y_test, lr_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Accuracy Scores - Test Set: 0.83280(+/- 0.05)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores_LRtest = cross_val_score(lr, X_test, y_test, cv=10)\n",
    "print('Cross Validation Accuracy Scores - Test Set: {:.5f}(+/- {:.2f})'.format(scores_LRtest.mean(), \n",
    "                                                                               scores_LRtest.std()*2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Model ##\n",
    "Lists of sentences were created were created for each corpus of lyrics.  A train/test split was conducted on each list of sentences and then each sentence was converted into a vector through the TfidfVectorizer package of sklearn.  Then a dataframe was created on the vectors of the lists of sentences.  Then the number of features was reduced to 130 features.  The vectorizer was done on the test data also to see if the vectorizer found the same sentences that were representative of the artist.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "adele_sents_list = list(adele_doc.sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Looking for some education\n",
       "Made my way into the night"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adele_sents_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adele_sentences = []\n",
    "for sent in adele_sents_list:\n",
    "    sentence = str(sent)\n",
    "    sentence = [re.sub('(\\\\n)', ' ', word) for word in sentence]\n",
    "    adele_sentences.append(''.join(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Looking for some education Made my way into the night ',\n",
       " \"All that bullshit conversation Baby, can't you read the signs?\",\n",
       " \"I won't bore you with the details, baby \",\n",
       " \"I don't even wanna waste your time \",\n",
       " \"Let's just say that maybe You could help me ease my mind \"]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adele_sentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Looking for some education Made my way into the night '"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adele_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "X_train, X_test = train_test_split(adele_sentences, test_size=0.4, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_df=0.5, # drop words that occur in more than half the paragraphs\n",
    "                             min_df=2, # only use words that appear at least twice\n",
    "                             stop_words='english', \n",
    "                             lowercase=True, #convert everything to lower case (since Alice in Wonderland has the HABIT of CAPITALIZING WORDS for EMPHASIS)\n",
    "                             use_idf=True,#we definitely want to use inverse document frequencies in our weighting\n",
    "                             norm=u'l2', #Applies a correction factor so that longer paragraphs and shorter paragraphs get treated equally\n",
    "                             smooth_idf=True #Adds 1 to all document frequencies, as if an extra document existed that used every word once.  Prevents divide-by-zero errors\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 707\n"
     ]
    }
   ],
   "source": [
    "#Applying the vectorizer\n",
    "adele_sents_tfidf=vectorizer.fit_transform(adele_sentences)\n",
    "print(\"Number of features: %d\" % adele_sents_tfidf.get_shape()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence: I'm going back to where I started The morning rain, the morning rain \n",
      "Tf_idf vector: {'started': 0.39069846463908886, 'going': 0.37592494534712279, 'morning': 0.62318203367003577, 'rain': 0.56363046236457071}\n"
     ]
    }
   ],
   "source": [
    "#splitting into training and test sets\n",
    "X_train_tfidf, X_test_tfidf= train_test_split(adele_sents_tfidf, test_size=0.4, random_state=0)\n",
    "\n",
    "\n",
    "#Reshapes the vectorizer output into something people can read\n",
    "X_train_tfidf_csr = X_train_tfidf.tocsr()\n",
    "\n",
    "#number of paragraphs\n",
    "n = X_train_tfidf_csr.shape[0]\n",
    "#A list of dictionaries, one per paragraph\n",
    "tfidf_bypara = [{} for _ in range(0,n)]\n",
    "#List of features\n",
    "terms = vectorizer.get_feature_names()\n",
    "#for each paragraph, lists the feature words and their tf-idf scores\n",
    "for i, j in zip(*X_train_tfidf_csr.nonzero()):\n",
    "    tfidf_bypara[i][terms[j]] = X_train_tfidf_csr[i, j]\n",
    "\n",
    "#Keep in mind that the log base 2 of 1 is 0, so a tf-idf score of 0 indicates that the word was present once in that sentence.\n",
    "print('Original sentence:', X_train[5])\n",
    "print('Tf_idf vector:', tfidf_bypara[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent variance captured by all components: 68.0533549241\n",
      "Component 0:\n",
      "I'm the only one, in love                    0.94885\n",
      "If I'm in love with you Should I give up     0.94885\n",
      "I will always love you                       0.94885\n",
      "I'm the only one in love                     0.94885\n",
      "Why do you love me, do you love me?          0.94885\n",
      "I will always love you                       0.94885\n",
      "Why do you love me, do you love me?          0.94885\n",
      "I'm the only one in love                     0.94885\n",
      "Do you love me?                              0.94885\n",
      "Love to me,                                  0.94885\n",
      "Name: 0, dtype: float64\n",
      "Component 1:\n",
      "We've both know we ain't kids no more     0.590701\n",
      "We've both know we ain't kids no more     0.590701\n",
      "But some of us don't know why I           0.548503\n",
      "Do you even know that I can't let go      0.535549\n",
      "Do you even know that I can't let go      0.535549\n",
      "I've been here before                     0.526121\n",
      "Could've had it all...                    0.526121\n",
      "For everything that I've done             0.526121\n",
      "You've found yourself                     0.526121\n",
      "For everything that I've done             0.526121\n",
      "Name: 1, dtype: float64\n",
      "Component 2:\n",
      "We could've had it all...                                 0.716511\n",
      "I've been here before                                     0.716511\n",
      "You've found yourself                                     0.716511\n",
      "Could've had it all...                                    0.716511\n",
      "For everything that I've done                             0.716511\n",
      "We could've had it all...                                 0.716511\n",
      "For everything that I've done                             0.716511\n",
      "You've found yourself                                     0.716511\n",
      "I've fallen off the edge to find that I've gone blind     0.711144\n",
      "I've imagined it all                                      0.708108\n",
      "Name: 2, dtype: float64\n",
      "Component 3:\n",
      "We both know we ain't kids no more     0.744574\n",
      "We both know we ain't kids no more     0.744574\n",
      "We both know we ain't kids no more     0.744574\n",
      "We both know we ain't kids no more     0.744574\n",
      "We both know we ain't kids no more     0.744574\n",
      "We both know we ain't kids no more     0.744574\n",
      "We both know we ain't kids no more     0.744574\n",
      "We both know we ain't kids no more     0.744574\n",
      "We both know we ain't kids no more     0.744574\n",
      "I ain't                                0.633604\n",
      "Name: 3, dtype: float64\n",
      "Component 4:\n",
      "Ooh, ooh Baby, baby                                                                                                                                                                                                                   0.757062\n",
      "Ooh, ooh Baby, baby                                                                                                                                                                                                                   0.757062\n",
      "Ooh, ooh Baby, baby                                                                                                                                                                                                                   0.757062\n",
      "ooh Rumor has it,                                                                                                                                                                                                                     0.754957\n",
      "ooh Rumor has it,                                                                                                                                                                                                                     0.754957\n",
      "ooh Rumor has it,                                                                                                                                                                                                                     0.754957\n",
      "ooh Rumor has it,                                                                                                                                                                                                                     0.754957\n",
      "Rumor has it, ooh, Rumor has it, ooh, Rumor has it, ooh, Rumor has it, ooh, Rumor has it, ooh, Rumor has it, ooh, Rumor has it, ooh, Rumor has it, ooh, Rumor has it, ooh, Rumor has it, ooh, Rumor has it, ooh, Rumor has it, ooh    0.754957\n",
      "ooh Rumor has it,                                                                                                                                                                                                                     0.754957\n",
      "ooh Rumor has it,                                                                                                                                                                                                                     0.754957\n",
      "Name: 4, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "#Our SVD data reducer.  We are going to reduce the feature space from 1379 to 130.\n",
    "svd= TruncatedSVD(130)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "# Run SVD on the training data, then project the training data.\n",
    "X_train_lsa = lsa.fit_transform(X_train_tfidf)\n",
    "\n",
    "variance_explained=svd.explained_variance_ratio_\n",
    "total_variance = variance_explained.sum()\n",
    "print(\"Percent variance captured by all components:\",total_variance*100)\n",
    "\n",
    "#Looking at what sorts of sentences our solution considers similar, for the first five identified topics\n",
    "adele_train=pd.DataFrame(X_train_lsa,index=X_train)\n",
    "for i in range(5):\n",
    "    print('Component {}:'.format(i))\n",
    "    print(adele_train.loc[:,i].sort_values(ascending=False)[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent variance captured by all components: 68.7490463061\n",
      "Component 0:\n",
      "Why do you love me, do you love me?     0.946444\n",
      "Why do you love me?                     0.946444\n",
      "I will always love you                  0.946444\n",
      "Love to you,                            0.946444\n",
      "Why do you love me, do you love me?     0.946444\n",
      "Love to you,                            0.946444\n",
      "Love you                                0.946444\n",
      "Love to you,                            0.946444\n",
      "Do you love me?                         0.946444\n",
      "I'm the only one in love                0.946444\n",
      "Name: 0, dtype: float64\n",
      "Component 1:\n",
      "We both know we ain't kids no more     0.544262\n",
      "We both know we ain't kids no more     0.544262\n",
      "We both know we ain't kids no more     0.544262\n",
      "We both know we ain't kids no more     0.544262\n",
      "We both know we ain't kids no more     0.544262\n",
      "We both know we ain't kids no more     0.544262\n",
      "We both know we ain't kids no more     0.544262\n",
      "Say it ain't so, say it ain't so       0.538024\n",
      ", say it ain't so                      0.538024\n",
      "Say it ain't so, say it ain't so       0.538024\n",
      "Name: 1, dtype: float64\n",
      "Component 2:\n",
      "ooh baby                0.720686\n",
      "ooh baby                0.720686\n",
      "Ooh, ooh Baby, baby     0.720686\n",
      "Ooh, ooh                0.646680\n",
      "Ooh, ooh                0.646680\n",
      "Ooh                     0.646680\n",
      "ooh                     0.646680\n",
      "Ooh, ooh                0.646680\n",
      "Ooh, ooh                0.646680\n",
      "Ooh                     0.646680\n",
      "Name: 2, dtype: float64\n",
      "Component 3:\n",
      "yeah     0.985803\n",
      "yeah     0.985803\n",
      "-yeah    0.985803\n",
      "yeah     0.985803\n",
      "yeah     0.985803\n",
      "-yeah    0.985803\n",
      "yeah     0.985803\n",
      "yeah     0.985803\n",
      "yeah     0.985803\n",
      "yeah     0.985803\n",
      "Name: 3, dtype: float64\n",
      "Component 4:\n",
      "Ooh                     0.528030\n",
      "Ooh                     0.528030\n",
      "Ooh, ooh                0.528030\n",
      "Ooh, ooh                0.528030\n",
      "ooh                     0.528030\n",
      "Ooh, ooh                0.528030\n",
      "Ooh, ooh                0.528030\n",
      "Ooh, ooh Baby, baby     0.460253\n",
      "ooh baby                0.460253\n",
      "ooh baby                0.460253\n",
      "Name: 4, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "#Our SVD data reducer.  We are going to reduce the feature space from 1379 to 130.\n",
    "svd= TruncatedSVD(130)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "\n",
    "# Remember, you will use the same model, only with the test set data.  Don't fit a new model by mistake!\n",
    "# Run SVD on the training data, then project the training data.\n",
    "X_test_lsa = lsa.fit_transform(X_test_tfidf)\n",
    "\n",
    "variance_explained=svd.explained_variance_ratio_\n",
    "total_variance = variance_explained.sum()\n",
    "print(\"Percent variance captured by all components:\",total_variance*100)\n",
    "\n",
    "#Looking at what sorts of sentences our solution considers similar, for the first five identified topics\n",
    "adele_test=pd.DataFrame(X_test_lsa,index=X_test)\n",
    "for i in range(5):\n",
    "    print('Component {}:'.format(i))\n",
    "    print(adele_test.loc[:,i].sort_values(ascending=False)[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "adele_train['Singer'] = 'Adele'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "      <th>128</th>\n",
       "      <th>129</th>\n",
       "      <th>Singer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>It's above and beyond me</th>\n",
       "      <td>0.024201</td>\n",
       "      <td>0.056962</td>\n",
       "      <td>-0.058908</td>\n",
       "      <td>-0.013320</td>\n",
       "      <td>-0.077640</td>\n",
       "      <td>0.021372</td>\n",
       "      <td>-0.020693</td>\n",
       "      <td>0.039779</td>\n",
       "      <td>0.038830</td>\n",
       "      <td>-0.092165</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066965</td>\n",
       "      <td>0.066910</td>\n",
       "      <td>-0.002436</td>\n",
       "      <td>0.010367</td>\n",
       "      <td>0.010694</td>\n",
       "      <td>-0.000349</td>\n",
       "      <td>0.054896</td>\n",
       "      <td>0.056482</td>\n",
       "      <td>0.007767</td>\n",
       "      <td>Adele</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is the end I've drowned and dreamt this moment</th>\n",
       "      <td>0.016981</td>\n",
       "      <td>0.220528</td>\n",
       "      <td>0.297538</td>\n",
       "      <td>-0.081648</td>\n",
       "      <td>-0.004936</td>\n",
       "      <td>-0.030301</td>\n",
       "      <td>0.040969</td>\n",
       "      <td>-0.076614</td>\n",
       "      <td>0.013669</td>\n",
       "      <td>0.041880</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003845</td>\n",
       "      <td>0.014246</td>\n",
       "      <td>-0.018926</td>\n",
       "      <td>-0.038748</td>\n",
       "      <td>-0.098381</td>\n",
       "      <td>0.044020</td>\n",
       "      <td>0.000890</td>\n",
       "      <td>0.084948</td>\n",
       "      <td>0.070834</td>\n",
       "      <td>Adele</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>But like everything I've ever known You disappear one day</th>\n",
       "      <td>0.022684</td>\n",
       "      <td>0.272189</td>\n",
       "      <td>0.314003</td>\n",
       "      <td>-0.088766</td>\n",
       "      <td>-0.031810</td>\n",
       "      <td>-0.016091</td>\n",
       "      <td>0.110070</td>\n",
       "      <td>-0.184393</td>\n",
       "      <td>0.072353</td>\n",
       "      <td>-0.005150</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016655</td>\n",
       "      <td>-0.005175</td>\n",
       "      <td>-0.043007</td>\n",
       "      <td>0.003413</td>\n",
       "      <td>0.059052</td>\n",
       "      <td>0.019415</td>\n",
       "      <td>-0.021103</td>\n",
       "      <td>-0.007212</td>\n",
       "      <td>0.024681</td>\n",
       "      <td>Adele</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>It illuminates all of my doubts Pull me in, hold me tight</th>\n",
       "      <td>0.002327</td>\n",
       "      <td>0.007205</td>\n",
       "      <td>-0.004754</td>\n",
       "      <td>-0.001776</td>\n",
       "      <td>-0.002344</td>\n",
       "      <td>0.007492</td>\n",
       "      <td>0.006161</td>\n",
       "      <td>-0.010512</td>\n",
       "      <td>0.008446</td>\n",
       "      <td>-0.012808</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002398</td>\n",
       "      <td>0.072589</td>\n",
       "      <td>-0.084735</td>\n",
       "      <td>0.035143</td>\n",
       "      <td>0.004223</td>\n",
       "      <td>0.042171</td>\n",
       "      <td>-0.019861</td>\n",
       "      <td>0.027534</td>\n",
       "      <td>-0.076979</td>\n",
       "      <td>Adele</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>And watched you wave</th>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000862</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>-0.000688</td>\n",
       "      <td>-0.000120</td>\n",
       "      <td>0.000796</td>\n",
       "      <td>-0.000958</td>\n",
       "      <td>0.003879</td>\n",
       "      <td>0.000232</td>\n",
       "      <td>-0.005266</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.027109</td>\n",
       "      <td>-0.074369</td>\n",
       "      <td>-0.043441</td>\n",
       "      <td>-0.027245</td>\n",
       "      <td>-0.009218</td>\n",
       "      <td>0.055866</td>\n",
       "      <td>0.107946</td>\n",
       "      <td>-0.004781</td>\n",
       "      <td>-0.054873</td>\n",
       "      <td>Adele</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 131 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                           0         1  \\\n",
       "It's above and beyond me                            0.024201  0.056962   \n",
       "is the end I've drowned and dreamt this moment      0.016981  0.220528   \n",
       "But like everything I've ever known You disappe...  0.022684  0.272189   \n",
       "It illuminates all of my doubts Pull me in, hol...  0.002327  0.007205   \n",
       "And watched you wave                                0.000034  0.000862   \n",
       "\n",
       "                                                           2         3  \\\n",
       "It's above and beyond me                           -0.058908 -0.013320   \n",
       "is the end I've drowned and dreamt this moment      0.297538 -0.081648   \n",
       "But like everything I've ever known You disappe...  0.314003 -0.088766   \n",
       "It illuminates all of my doubts Pull me in, hol... -0.004754 -0.001776   \n",
       "And watched you wave                                0.000011 -0.000688   \n",
       "\n",
       "                                                           4         5  \\\n",
       "It's above and beyond me                           -0.077640  0.021372   \n",
       "is the end I've drowned and dreamt this moment     -0.004936 -0.030301   \n",
       "But like everything I've ever known You disappe... -0.031810 -0.016091   \n",
       "It illuminates all of my doubts Pull me in, hol... -0.002344  0.007492   \n",
       "And watched you wave                               -0.000120  0.000796   \n",
       "\n",
       "                                                           6         7  \\\n",
       "It's above and beyond me                           -0.020693  0.039779   \n",
       "is the end I've drowned and dreamt this moment      0.040969 -0.076614   \n",
       "But like everything I've ever known You disappe...  0.110070 -0.184393   \n",
       "It illuminates all of my doubts Pull me in, hol...  0.006161 -0.010512   \n",
       "And watched you wave                               -0.000958  0.003879   \n",
       "\n",
       "                                                           8         9  \\\n",
       "It's above and beyond me                            0.038830 -0.092165   \n",
       "is the end I've drowned and dreamt this moment      0.013669  0.041880   \n",
       "But like everything I've ever known You disappe...  0.072353 -0.005150   \n",
       "It illuminates all of my doubts Pull me in, hol...  0.008446 -0.012808   \n",
       "And watched you wave                                0.000232 -0.005266   \n",
       "\n",
       "                                                     ...         121  \\\n",
       "It's above and beyond me                             ...    0.066965   \n",
       "is the end I've drowned and dreamt this moment       ...   -0.003845   \n",
       "But like everything I've ever known You disappe...   ...    0.016655   \n",
       "It illuminates all of my doubts Pull me in, hol...   ...   -0.002398   \n",
       "And watched you wave                                 ...   -0.027109   \n",
       "\n",
       "                                                         122       123  \\\n",
       "It's above and beyond me                            0.066910 -0.002436   \n",
       "is the end I've drowned and dreamt this moment      0.014246 -0.018926   \n",
       "But like everything I've ever known You disappe... -0.005175 -0.043007   \n",
       "It illuminates all of my doubts Pull me in, hol...  0.072589 -0.084735   \n",
       "And watched you wave                               -0.074369 -0.043441   \n",
       "\n",
       "                                                         124       125  \\\n",
       "It's above and beyond me                            0.010367  0.010694   \n",
       "is the end I've drowned and dreamt this moment     -0.038748 -0.098381   \n",
       "But like everything I've ever known You disappe...  0.003413  0.059052   \n",
       "It illuminates all of my doubts Pull me in, hol...  0.035143  0.004223   \n",
       "And watched you wave                               -0.027245 -0.009218   \n",
       "\n",
       "                                                         126       127  \\\n",
       "It's above and beyond me                           -0.000349  0.054896   \n",
       "is the end I've drowned and dreamt this moment      0.044020  0.000890   \n",
       "But like everything I've ever known You disappe...  0.019415 -0.021103   \n",
       "It illuminates all of my doubts Pull me in, hol...  0.042171 -0.019861   \n",
       "And watched you wave                                0.055866  0.107946   \n",
       "\n",
       "                                                         128       129  Singer  \n",
       "It's above and beyond me                            0.056482  0.007767   Adele  \n",
       "is the end I've drowned and dreamt this moment      0.084948  0.070834   Adele  \n",
       "But like everything I've ever known You disappe... -0.007212  0.024681   Adele  \n",
       "It illuminates all of my doubts Pull me in, hol...  0.027534 -0.076979   Adele  \n",
       "And watched you wave                               -0.004781 -0.054873   Adele  \n",
       "\n",
       "[5 rows x 131 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adele_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "marley_sents_list = list(marley_doc.sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Don't worry about a thing,\n",
       "'Cause every little thing gonna be all right."
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "marley_sents_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "marley_sentences = []\n",
    "for sent in marley_sents_list:\n",
    "    sentence = str(sent)\n",
    "    sentence = [re.sub('(\\\\n)', ' ', word) for word in sentence]\n",
    "    #sentence = [re.sub('(\\\\)', '', word) for word in sentence]\n",
    "    marley_sentences.append(''.join(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(marley_sentences, test_size=0.4, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 776\n"
     ]
    }
   ],
   "source": [
    "#Applying the vectorizer\n",
    "marley_sents_tfidf=vectorizer.fit_transform(marley_sentences)\n",
    "print(\"Number of features: %d\" % marley_sents_tfidf.get_shape()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence: We're past the worse Hypocrites and parasites Will come up and take a bite \n",
      "Tf_idf vector: {'hypocrites': 0.66851004783734169, 'past': 0.61729668647951841, 'come': 0.41477598387795117}\n"
     ]
    }
   ],
   "source": [
    "#splitting into training and test sets\n",
    "X_train_tfidf, X_test_tfidf= train_test_split(marley_sents_tfidf, test_size=0.4, random_state=0)\n",
    "\n",
    "\n",
    "#Reshapes the vectorizer output into something people can read\n",
    "X_train_tfidf_csr = X_train_tfidf.tocsr()\n",
    "\n",
    "#number of paragraphs\n",
    "n = X_train_tfidf_csr.shape[0]\n",
    "#A list of dictionaries, one per paragraph\n",
    "tfidf_bypara = [{} for _ in range(0,n)]\n",
    "#List of features\n",
    "terms = vectorizer.get_feature_names()\n",
    "#for each paragraph, lists the feature words and their tf-idf scores\n",
    "for i, j in zip(*X_train_tfidf_csr.nonzero()):\n",
    "    tfidf_bypara[i][terms[j]] = X_train_tfidf_csr[i, j]\n",
    "\n",
    "#Keep in mind that the log base 2 of 1 is 0, so a tf-idf score of 0 indicates that the word was present once in that sentence.\n",
    "print('Original sentence:', X_train[6])\n",
    "print('Tf_idf vector:', tfidf_bypara[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent variance captured by all components: 69.7760318581\n",
      "Component 0:\n",
      "yeah:                     0.995375\n",
      "yeah                      0.995375\n",
      "-yeah,                    0.995375\n",
      "yeah                      0.995375\n",
      "yeah                      0.995375\n",
      "Where we can be, yeah.    0.995375\n",
      "yeah                      0.995375\n",
      "yeah,                     0.995375\n",
      "Yeah,                     0.995375\n",
      "yeah                      0.995375\n",
      "Name: 0, dtype: float64\n",
      "Component 1:\n",
      "Oh, no!     0.869609\n",
      "Oh-oh!      0.869609\n",
      "Oh I,       0.869609\n",
      "oh no!      0.869609\n",
      "Oh I,       0.869609\n",
      "oh oh!      0.869609\n",
      "Oh now!     0.869609\n",
      "-oh!        0.869609\n",
      "oh I,       0.869609\n",
      "oh oh!      0.869609\n",
      "Name: 1, dtype: float64\n",
      "Component 2:\n",
      "Everything's gonna be all right!     0.598775\n",
      "Everything's gonna be all right!     0.598775\n",
      "Everything's gonna be all right!     0.598775\n",
      "Everything's gonna be all right!     0.598775\n",
      "Everything's gonna be all right!     0.598775\n",
      "Everything's gonna be all right!     0.598775\n",
      "Everything's gonna be all right!     0.598775\n",
      "Everything's gonna be all right!     0.598775\n",
      "Everything's gonna be all right!     0.598775\n",
      "Everything's gonna be all right!     0.598775\n",
      "Name: 2, dtype: float64\n",
      "Component 3:\n",
      "Love you - love you, baby, so much.                    0.632433\n",
      "I love you so much, so-o-o-                            0.604460\n",
      "(One Love!);                                           0.604460\n",
      "One Love!                                              0.604460\n",
      "Is this love, is this love, is this love               0.604460\n",
      "(I love you, I love you)                               0.604460\n",
      "Love was at your first sight                           0.599225\n",
      "Send me that love                                      0.590296\n",
      "There is someone who needs to love                     0.585033\n",
      "Our love needs protection Our love needs direction     0.585033\n",
      "Name: 3, dtype: float64\n",
      "Component 4:\n",
      ", I don't wanna                                  0.769440\n",
      "(I don't wanna,                                  0.769440\n",
      "I don't wanna                                    0.769440\n",
      ", I don't wanna                                  0.769440\n",
      "I don't wanna                                    0.769440\n",
      "(I don't wanna, I don't wanna, I don't wanna     0.769440\n",
      "I don't wanna                                    0.769440\n",
      "No, I don't wanna                                0.769440\n",
      "I don't wanna, I don't wanna wait in vain)       0.751725\n",
      "I don't wanna, I don't wanna wait in vain)       0.751725\n",
      "Name: 4, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Our SVD data reducer.  We are going to reduce the feature space from 1379 to 130.\n",
    "svd= TruncatedSVD(130)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "# Run SVD on the training data, then project the training data.\n",
    "X_train_lsa = lsa.fit_transform(X_train_tfidf)\n",
    "\n",
    "variance_explained=svd.explained_variance_ratio_\n",
    "total_variance = variance_explained.sum()\n",
    "print(\"Percent variance captured by all components:\",total_variance*100)\n",
    "\n",
    "#Looking at what sorts of paragraphs our solution considers similar, for the first five identified topics\n",
    "marley_train=pd.DataFrame(X_train_lsa,index=X_train)\n",
    "for i in range(5):\n",
    "    print('Component {}:'.format(i))\n",
    "    print(marley_train.loc[:,i].sort_values(ascending=False)[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent variance captured by all components: 72.0644840204\n",
      "Component 0:\n",
      "yeah,        0.939341\n",
      "yeah         0.939341\n",
      "yeah         0.939341\n",
      "yeah,        0.939341\n",
      "yeah)        0.939341\n",
      "yeah         0.939341\n",
      "yeah:        0.939341\n",
      "yeah,        0.939341\n",
      "yeah-yeah    0.939341\n",
      "yeah         0.939341\n",
      "Name: 0, dtype: float64\n",
      "Component 1:\n",
      "oh I,         0.931365\n",
      "oh I,         0.931365\n",
      "O-oh!         0.931365\n",
      "-oh!          0.931365\n",
      "oh            0.931365\n",
      "Oh (part),    0.931365\n",
      "O-oh!         0.931365\n",
      "oh,           0.931365\n",
      "Oh, no!       0.931365\n",
      "oh-oh,        0.931365\n",
      "Name: 1, dtype: float64\n",
      "Component 2:\n",
      "Let's get together and feel all right                     0.776416\n",
      "Let's get together and feel all right                     0.776416\n",
      "Let's get together and feel all right                     0.776416\n",
      "Let's get together and feel all right                     0.776416\n",
      "Let's get together and feel all right                     0.776416\n",
      "What about the, let's get together and feel all right     0.776416\n",
      "and I will feel all right                                 0.759557\n",
      "and I will feel all right                                 0.759557\n",
      "and I will feel all right                                 0.759557\n",
      "and I will feel all right                                 0.759557\n",
      "Name: 2, dtype: float64\n",
      "Component 3:\n",
      "but you baby)      0.974176\n",
      "baby               0.974176\n",
      "but you baby (     0.974176\n",
      "baby               0.974176\n",
      "but you baby (     0.974176\n",
      "but you baby)      0.974176\n",
      "Here I am baby     0.974176\n",
      "but you baby)      0.974176\n",
      "baby               0.974176\n",
      "but you baby)      0.974176\n",
      "Name: 3, dtype: float64\n",
      "Component 4:\n",
      "It's your love that I'm waiting on (I don't wanna)     0.707341\n",
      "I don't wanna wait in vain for your love,              0.665523\n",
      "I don't wanna wait in vain for your love               0.665523\n",
      "I don't wanna wait in vain for your love               0.665523\n",
      "I don't wanna,                                         0.620689\n",
      "No, I don't wanna,                                     0.620689\n",
      ", I don't wanna                                        0.620689\n",
      ", I don't wanna                                        0.620689\n",
      "I don't wanna,                                         0.620689\n",
      "I don't wanna                                          0.620689\n",
      "Name: 4, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Remember, you will use the same model, only with the test set data.  Don't fit a new model by mistake!\n",
    "# Run SVD on the training data, then project the training data.\n",
    "X_test_lsa = lsa.fit_transform(X_test_tfidf)\n",
    "\n",
    "variance_explained=svd.explained_variance_ratio_\n",
    "total_variance = variance_explained.sum()\n",
    "print(\"Percent variance captured by all components:\",total_variance*100)\n",
    "\n",
    "#Looking at what sorts of paragraphs our solution considers similar, for the first five identified topics\n",
    "marley_test=pd.DataFrame(X_test_lsa,index=X_test)\n",
    "for i in range(5):\n",
    "    print('Component {}:'.format(i))\n",
    "    print(marley_test.loc[:,i].sort_values(ascending=False)[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "marley_train['Singer'] = 'Bob Marley'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "      <th>128</th>\n",
       "      <th>129</th>\n",
       "      <th>Singer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Uh, open your eyes and look within</th>\n",
       "      <td>0.002953</td>\n",
       "      <td>0.014961</td>\n",
       "      <td>0.025718</td>\n",
       "      <td>0.002279</td>\n",
       "      <td>0.009936</td>\n",
       "      <td>-0.005375</td>\n",
       "      <td>-0.003413</td>\n",
       "      <td>0.026742</td>\n",
       "      <td>-0.005507</td>\n",
       "      <td>-0.008019</td>\n",
       "      <td>...</td>\n",
       "      <td>0.082938</td>\n",
       "      <td>-0.065735</td>\n",
       "      <td>-0.110302</td>\n",
       "      <td>-0.108918</td>\n",
       "      <td>0.015582</td>\n",
       "      <td>0.083468</td>\n",
       "      <td>-0.035502</td>\n",
       "      <td>0.074635</td>\n",
       "      <td>0.033000</td>\n",
       "      <td>Bob Marley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Could you be, could you be loved?</th>\n",
       "      <td>0.000254</td>\n",
       "      <td>0.008223</td>\n",
       "      <td>0.014810</td>\n",
       "      <td>-0.007188</td>\n",
       "      <td>-0.009914</td>\n",
       "      <td>0.029387</td>\n",
       "      <td>-0.058984</td>\n",
       "      <td>0.341906</td>\n",
       "      <td>0.134186</td>\n",
       "      <td>-0.304527</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007373</td>\n",
       "      <td>-0.001196</td>\n",
       "      <td>0.002151</td>\n",
       "      <td>0.005227</td>\n",
       "      <td>0.004731</td>\n",
       "      <td>0.005259</td>\n",
       "      <td>-0.001507</td>\n",
       "      <td>0.012185</td>\n",
       "      <td>0.007304</td>\n",
       "      <td>Bob Marley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>This fire (fire), this fire (fire)</th>\n",
       "      <td>0.000397</td>\n",
       "      <td>-0.091070</td>\n",
       "      <td>0.062259</td>\n",
       "      <td>-0.042070</td>\n",
       "      <td>-0.169850</td>\n",
       "      <td>0.269276</td>\n",
       "      <td>-0.024397</td>\n",
       "      <td>0.217806</td>\n",
       "      <td>0.072186</td>\n",
       "      <td>-0.087453</td>\n",
       "      <td>...</td>\n",
       "      <td>0.047491</td>\n",
       "      <td>-0.007575</td>\n",
       "      <td>0.058156</td>\n",
       "      <td>-0.086841</td>\n",
       "      <td>0.005824</td>\n",
       "      <td>-0.017446</td>\n",
       "      <td>-0.116623</td>\n",
       "      <td>0.006207</td>\n",
       "      <td>-0.032898</td>\n",
       "      <td>Bob Marley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>solo/ So we know we can't take your slogans no more, can't take your slogans no more, can't take your slogans no more, no more sweet talk from-a pulpit, no more sweet talk from the pulpit.</th>\n",
       "      <td>0.002553</td>\n",
       "      <td>0.029477</td>\n",
       "      <td>0.031844</td>\n",
       "      <td>0.018572</td>\n",
       "      <td>0.065032</td>\n",
       "      <td>0.026903</td>\n",
       "      <td>0.156391</td>\n",
       "      <td>0.035779</td>\n",
       "      <td>-0.005263</td>\n",
       "      <td>0.000502</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019015</td>\n",
       "      <td>0.008133</td>\n",
       "      <td>0.029260</td>\n",
       "      <td>0.015358</td>\n",
       "      <td>0.029315</td>\n",
       "      <td>0.055681</td>\n",
       "      <td>0.004294</td>\n",
       "      <td>0.082943</td>\n",
       "      <td>0.003542</td>\n",
       "      <td>Bob Marley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Every minute All you got to do, baby, (oh-oh-oh-oh) Is keep it in, and (Stir it up)</th>\n",
       "      <td>0.065974</td>\n",
       "      <td>0.846715</td>\n",
       "      <td>-0.318500</td>\n",
       "      <td>-0.066196</td>\n",
       "      <td>-0.112346</td>\n",
       "      <td>0.116945</td>\n",
       "      <td>-0.017961</td>\n",
       "      <td>0.041308</td>\n",
       "      <td>-0.107682</td>\n",
       "      <td>0.090868</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009377</td>\n",
       "      <td>-0.009207</td>\n",
       "      <td>-0.017066</td>\n",
       "      <td>0.000714</td>\n",
       "      <td>0.014579</td>\n",
       "      <td>0.005314</td>\n",
       "      <td>-0.006128</td>\n",
       "      <td>0.013422</td>\n",
       "      <td>-0.009485</td>\n",
       "      <td>Bob Marley</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 131 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                           0         1  \\\n",
       "Uh, open your eyes and look within                  0.002953  0.014961   \n",
       "Could you be, could you be loved?                   0.000254  0.008223   \n",
       "This fire (fire), this fire (fire)                  0.000397 -0.091070   \n",
       "solo/ So we know we can't take your slogans no ...  0.002553  0.029477   \n",
       "Every minute All you got to do, baby, (oh-oh-oh...  0.065974  0.846715   \n",
       "\n",
       "                                                           2         3  \\\n",
       "Uh, open your eyes and look within                  0.025718  0.002279   \n",
       "Could you be, could you be loved?                   0.014810 -0.007188   \n",
       "This fire (fire), this fire (fire)                  0.062259 -0.042070   \n",
       "solo/ So we know we can't take your slogans no ...  0.031844  0.018572   \n",
       "Every minute All you got to do, baby, (oh-oh-oh... -0.318500 -0.066196   \n",
       "\n",
       "                                                           4         5  \\\n",
       "Uh, open your eyes and look within                  0.009936 -0.005375   \n",
       "Could you be, could you be loved?                  -0.009914  0.029387   \n",
       "This fire (fire), this fire (fire)                 -0.169850  0.269276   \n",
       "solo/ So we know we can't take your slogans no ...  0.065032  0.026903   \n",
       "Every minute All you got to do, baby, (oh-oh-oh... -0.112346  0.116945   \n",
       "\n",
       "                                                           6         7  \\\n",
       "Uh, open your eyes and look within                 -0.003413  0.026742   \n",
       "Could you be, could you be loved?                  -0.058984  0.341906   \n",
       "This fire (fire), this fire (fire)                 -0.024397  0.217806   \n",
       "solo/ So we know we can't take your slogans no ...  0.156391  0.035779   \n",
       "Every minute All you got to do, baby, (oh-oh-oh... -0.017961  0.041308   \n",
       "\n",
       "                                                           8         9  \\\n",
       "Uh, open your eyes and look within                 -0.005507 -0.008019   \n",
       "Could you be, could you be loved?                   0.134186 -0.304527   \n",
       "This fire (fire), this fire (fire)                  0.072186 -0.087453   \n",
       "solo/ So we know we can't take your slogans no ... -0.005263  0.000502   \n",
       "Every minute All you got to do, baby, (oh-oh-oh... -0.107682  0.090868   \n",
       "\n",
       "                                                       ...           121  \\\n",
       "Uh, open your eyes and look within                     ...      0.082938   \n",
       "Could you be, could you be loved?                      ...     -0.007373   \n",
       "This fire (fire), this fire (fire)                     ...      0.047491   \n",
       "solo/ So we know we can't take your slogans no ...     ...     -0.019015   \n",
       "Every minute All you got to do, baby, (oh-oh-oh...     ...     -0.009377   \n",
       "\n",
       "                                                         122       123  \\\n",
       "Uh, open your eyes and look within                 -0.065735 -0.110302   \n",
       "Could you be, could you be loved?                  -0.001196  0.002151   \n",
       "This fire (fire), this fire (fire)                 -0.007575  0.058156   \n",
       "solo/ So we know we can't take your slogans no ...  0.008133  0.029260   \n",
       "Every minute All you got to do, baby, (oh-oh-oh... -0.009207 -0.017066   \n",
       "\n",
       "                                                         124       125  \\\n",
       "Uh, open your eyes and look within                 -0.108918  0.015582   \n",
       "Could you be, could you be loved?                   0.005227  0.004731   \n",
       "This fire (fire), this fire (fire)                 -0.086841  0.005824   \n",
       "solo/ So we know we can't take your slogans no ...  0.015358  0.029315   \n",
       "Every minute All you got to do, baby, (oh-oh-oh...  0.000714  0.014579   \n",
       "\n",
       "                                                         126       127  \\\n",
       "Uh, open your eyes and look within                  0.083468 -0.035502   \n",
       "Could you be, could you be loved?                   0.005259 -0.001507   \n",
       "This fire (fire), this fire (fire)                 -0.017446 -0.116623   \n",
       "solo/ So we know we can't take your slogans no ...  0.055681  0.004294   \n",
       "Every minute All you got to do, baby, (oh-oh-oh...  0.005314 -0.006128   \n",
       "\n",
       "                                                         128       129  \\\n",
       "Uh, open your eyes and look within                  0.074635  0.033000   \n",
       "Could you be, could you be loved?                   0.012185  0.007304   \n",
       "This fire (fire), this fire (fire)                  0.006207 -0.032898   \n",
       "solo/ So we know we can't take your slogans no ...  0.082943  0.003542   \n",
       "Every minute All you got to do, baby, (oh-oh-oh...  0.013422 -0.009485   \n",
       "\n",
       "                                                        Singer  \n",
       "Uh, open your eyes and look within                  Bob Marley  \n",
       "Could you be, could you be loved?                   Bob Marley  \n",
       "This fire (fire), this fire (fire)                  Bob Marley  \n",
       "solo/ So we know we can't take your slogans no ...  Bob Marley  \n",
       "Every minute All you got to do, baby, (oh-oh-oh...  Bob Marley  \n",
       "\n",
       "[5 rows x 131 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "marley_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Models on the DataFrame created by Unsupervised Modeling ###\n",
    "The dataframes of the vectorized sentences of each lyric corpus was combined to create one dataframe.  Then three supervised models were run on the dataframe to predict the singer based on the vectorized sentence.  The best performing models were the tree based models.  Gradient Boosting model was the best with a 98% accuracy on the test set.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combined_train = adele_train.append(marley_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.985507246377\n",
      "\n",
      "Test set score: 0.969754253308\n"
     ]
    }
   ],
   "source": [
    "rfc = ensemble.RandomForestClassifier()\n",
    "Y = combined_train['Singer']\n",
    "X = np.array(combined_train.drop(['Singer'], 1))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    Y,\n",
    "                                                    test_size=0.4,\n",
    "                                                    random_state=0)\n",
    "train = rfc.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score:', rfc.score(X_train, y_train))\n",
    "print('\\nTest set score:', rfc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1587, 130) (1587,)\n",
      "Training set score: 0.742281033396\n",
      "\n",
      "Test set score: 0.703213610586\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "train = lr.fit(X_train, y_train)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print('Training set score:', lr.score(X_train, y_train))\n",
    "print('\\nTest set score:', lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.985507246377\n",
      "\n",
      "Test set score: 0.973534971645\n"
     ]
    }
   ],
   "source": [
    "clf = ensemble.GradientBoostingClassifier()\n",
    "train = clf.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score:', clf.score(X_train, y_train))\n",
    "print('\\nTest set score:', clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Closer Look at the Best Supervised Model ###\n",
    "A Confusion matrix shows the number of correct and incorrect predictions on the test set.  Also, a cross-validation was run on the test set to make sure that no over-fitting was occuring.  The model was consistent across the ten folds of cross validation with a variance of +/- 4%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set score: 0.973534971645\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>Adele</th>\n",
       "      <th>Bob Marley</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Singer</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Adele</th>\n",
       "      <td>521</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bob Marley</th>\n",
       "      <td>22</td>\n",
       "      <td>509</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0       Adele  Bob Marley\n",
       "Singer                       \n",
       "Adele         521           6\n",
       "Bob Marley     22         509"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('\\nTest set score:', clf.score(X_test, y_test))\n",
    "clf_predicted = clf.predict(X_test)\n",
    "pd.crosstab(y_test, clf_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Accuracy Scores - Test Set: 0.97070(+/- 0.04)\n"
     ]
    }
   ],
   "source": [
    "scores_CLFtest = cross_val_score(clf, X_test, y_test, cv=10)\n",
    "print('Cross Validation Accuracy Scores - Test Set: {:.5f}(+/- {:.2f})'.format(scores_CLFtest.mean(), \n",
    "                                                                               scores_CLFtest.std()*2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
